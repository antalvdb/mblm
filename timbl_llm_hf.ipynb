{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antalvdb/mblm/blob/main/timbl_llm_hf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "tvCDBQZGNqO8"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import re\n",
        "import time\n",
        "import argparse\n",
        "import sys\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python3-timbl\n",
        "\n",
        "import timbl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ-xeliyOBgd",
        "outputId": "8daf6317-d5e5-4962-82a0-f48cacf6633b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python3-timbl in /usr/local/lib/python3.11/dist-packages (2025.1.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/antalvdb/mblm\n",
        "%cd mblm\n",
        "!git lfs pull -I chatbot-instruction-prompts_tok.l16r0.igtree.ibase\n",
        "%cd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5kZKfNs0Ru2",
        "outputId": "6b112253-3b91-455e-9f1f-c2b2de7ea243"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'mblm' already exists and is not an empty directory.\n",
            "/root/mblm\n",
            "/root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoConfig, AutoTokenizer, PreTrainedModel\n",
        "\n",
        "class TimblHuggingFaceModel(PreTrainedModel):\n",
        "\n",
        "    # Define a function to replace values with actual floats\n",
        "    def float_converter(match):\n",
        "        return f\"{match.group(1)}: {float(match.group(2))}\"\n",
        "\n",
        "    def __init__(self, config, timbl_classifier, tokenizer):\n",
        "        super().__init__(config)\n",
        "        self.timbl_classifier = timbl_classifier\n",
        "        self.tokenizer = tokenizer  # Store tokenizer\n",
        "\n",
        "    def forward(self, input_ids, **kwargs):\n",
        "\n",
        "        #print(\"inside forward\")\n",
        "\n",
        "        # Convert input_ids to Timbl format\n",
        "        timbl_input = self.convert_to_timbl_input(input_ids)\n",
        "        log(f\"Timbl input: {timbl_input}\",level=3)\n",
        "\n",
        "        # Get Timbl predictions\n",
        "        classlabel, distribution, distance = self.timbl_classifier.classify(timbl_input)\n",
        "        log(f\"Classlabel: {classlabel}\", level = 3)\n",
        "        log(f\"Distribution: {distribution}\", level = 3)\n",
        "        log(f\"Distance: {distance}\", level = 3\n",
        ")\n",
        "        # Convert Timbl output to Hugging Face format\n",
        "        logits = self.convert_to_huggingface_logits(distribution)\n",
        "        log(f\"Logits: {logits}\", level = 3)\n",
        "\n",
        "        # Return logits and other relevant outputs\n",
        "        return transformers.modeling_outputs.CausalLMOutputWithCrossAttentions(logits=logits)\n",
        "\n",
        "    def convert_to_timbl_input(self, input_ids):\n",
        "\n",
        "        #print(\"inside convert_to_timbl_input\")\n",
        "\n",
        "        \"\"\"Converts Hugging Face input_ids to Timbl input format.\"\"\"\n",
        "        # Decode input_ids to a string of tokens\n",
        "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
        "        log(f\"Tokens: {tokens}\", level = 3)\n",
        "\n",
        "        # Return the array of tokens directly\n",
        "        return tokens\n",
        "\n",
        "    def convert_to_huggingface_logits(self, distribution):\n",
        "\n",
        "        #print(\"inside convert_to_huggingface_logits\")\n",
        "\n",
        "        # Bypassing the typical HuggingFace device setting and passing\n",
        "        device = \"cpu\"\n",
        "\n",
        "        # Get vocabulary size from the tokenizer\n",
        "        vocab_size = self.tokenizer.vocab_size\n",
        "\n",
        "        # Initialize logits with a default value (e.g., -inf)\n",
        "        logits = torch.full((1, vocab_size), float('-inf'), device=device)\n",
        "        log(f\"Logits: {logits}\",level=3)\n",
        "\n",
        "        # Fill logits with probabilities from the Timbl distribution\n",
        "        for word, probability in distribution.items():\n",
        "            hf_token_id = self.tokenizer.convert_tokens_to_ids(word)\n",
        "            log(f\"hf_token_id: {hf_token_id}\", level = 4)\n",
        "\n",
        "            # Check if hf_token_id is a list and take the first element if it is\n",
        "            # Handling nested lists as well\n",
        "            while isinstance(hf_token_id, list) and len(hf_token_id) > 0:\n",
        "                hf_token_id = hf_token_id[0]\n",
        "                log(f\"hf_token_id: {hf_token_id}\", level = 4)\n",
        "\n",
        "            if isinstance(hf_token_id, int):  # Ensure it's now an integer\n",
        "                try:\n",
        "                    logits[0, hf_token_id] = torch.tensor(probability, device=device)\n",
        "                    log(f\"logits[0], hf_token_id]:  {logits[0, hf_token_id]} \", level = 4)\n",
        "                    log(f\"Logits shape: {logits.shape}\", level = 4)\n",
        "                except IndexError:\n",
        "                    # Handle the case where hf_token_id is out of bounds\n",
        "                    log(f\"Warning: Token ID {hf_token_id} is out of bounds for logits shape {logits.shape}\", level=1)\n",
        "            else:\n",
        "                log(f\"Warning: Skipping word '{word}' due to unexpected token ID format: {hf_token_id}\", level=1)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "yhwLJz13t1yq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global verbosity level\n",
        "VERBOSITY = 3"
      ],
      "metadata": {
        "id": "T2Oybcp2Scos"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log(message, level=1):\n",
        "    \"\"\"Logs a message if the verbosity level is sufficient.\"\"\"\n",
        "    if VERBOSITY >= level:\n",
        "        print(message)"
      ],
      "metadata": {
        "id": "_APEp2gxSoIJ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_prompt(words, max_len=16):\n",
        "    \"\"\"Pad or trim the list of words to make it exactly `max_len` words.\"\"\"\n",
        "    if words is None:\n",
        "        words = []  # Ensure words is a list\n",
        "    if len(words) < max_len:\n",
        "        words = ['_'] * (max_len - len(words)) + words\n",
        "    else:\n",
        "        words = words[-max_len:]\n",
        "    return words"
      ],
      "metadata": {
        "id": "1hG1i9xASuwt"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_from_api(model, tokenizer, initial_prompt, max_words=200):\n",
        "    # Tokenize the initial prompt and convert tokens back to words\n",
        "    initial_tokens = tokenizer.tokenize(initial_prompt)\n",
        "\n",
        "    if initial_tokens is None:\n",
        "        log(\"Tokenization failed; 'initial_tokens' is None.\", level=1)\n",
        "        initial_tokens = []\n",
        "\n",
        "    # Prepare the initial prompt, padded or trimmed to 16 words\n",
        "    padded_instances = []\n",
        "\n",
        "    # Generate padded instances for next-token predictions\n",
        "    for i in range(len(initial_tokens)):\n",
        "        # Take the tokens up to the current position and pad them\n",
        "        instance = pad_prompt(initial_tokens[:i], max_len=16)\n",
        "        padded_instances.append((instance, initial_tokens[i] if i < len(initial_tokens) else '_'))\n",
        "\n",
        "    # Add instances to memory using the TimblHuggingFaceModel (if applicable)\n",
        "    # This part might need to be adapted depending on how memory is handled in TimblHuggingFaceModel\n",
        "    for input_instance, next_token in padded_instances:\n",
        "        log(f\"memorized: {input_instance} {next_token}\", level=2)\n",
        "        # Adapt this line to call the appropriate method of TimblHuggingFaceModel for adding to memory\n",
        "        # model.append(input_instance, next_token)\n",
        "\n",
        "    # Use the final part of the prompt for further generation\n",
        "    prompt_words = pad_prompt(initial_tokens)\n",
        "\n",
        "    generated_tokens = prompt_words[:]  # Store the full generated text\n",
        "\n",
        "    try:\n",
        "        # Loop until max words generated or a period token is found\n",
        "        for _ in range(max_words):\n",
        "            next_word = None\n",
        "\n",
        "            # Get prediction from TimblHuggingFaceModel\n",
        "            encoded_input = tokenizer.encode(\" \".join(prompt_words), return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=16, add_special_tokens=False)\n",
        "            log(f\"encoded_input: {encoded_input}\", level = 3)\n",
        "            input_ids = encoded_input[0]  # Access the first sequence\n",
        "            log(f\"input_ids: {input_ids}\", level = 3)\n",
        "            outputs = model(input_ids) # get model output\n",
        "            log(f\"outputs: {outputs}\", level = 3)\n",
        "            logits = outputs.logits # extract logits\n",
        "            log(f\"logits: {logits}\", level = 3)\n",
        "\n",
        "            # Get the predicted token ID\n",
        "            # Get the predicted token ID, excluding the [CLS] token\n",
        "            predicted_token_id = torch.argmax(logits[:, 1:], dim=-1).item() + 1  # Add 1 to shift back to original index\n",
        "            #predicted_token_id = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "            # Decode the token ID to a word\n",
        "            classlabel = tokenizer.decode(predicted_token_id)\n",
        "\n",
        "            # Add instance to instance base (if applicable)\n",
        "            # Adapt this line to call the appropriate method of TimblHuggingFaceModel for adding to memory\n",
        "            # model.append(prompt_words, classlabel)\n",
        "\n",
        "            log(f\"Prompt words: {prompt_words}\", level=2)\n",
        "            log(f\"Classlabel: {classlabel}\", level=2)\n",
        "\n",
        "            generated_tokens.append(classlabel)\n",
        "\n",
        "            # Shift prompt words and add the new word\n",
        "            prompt_words = prompt_words[1:] + [classlabel]\n",
        "\n",
        "        # Detokenize the generated tokens\n",
        "        generated_text = tokenizer.convert_tokens_to_string(generated_tokens)\n",
        "\n",
        "        # Strip off original padding characters\n",
        "        generated_text = generated_text.replace(\"_\", \"\").strip()\n",
        "\n",
        "        # Print the final generated text\n",
        "        log(f\"Generated text: {generated_text}\", level=1)\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f\"Error: {e}\", level=1)"
      ],
      "metadata": {
        "id": "q7Gezwx0DMM4"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Simulate command-line arguments for notebook environment\n",
        "#    sys.argv = ['script_name', '--classifier', '/content/drive/MyDrive/dolly-15k-dutch-train_tok.l16r0', '--timbl_args', '-a4 +D', '--verbosity', '3']\n",
        "    sys.argv = ['script_name', '--classifier', '/content/mblm/chatbot-instruction-prompts_tok.l16r0.igtree', '--tokenizer', 'bert-base-cased', '--timbl_args', '-a1 +D', '--verbosity', '1']\n",
        "\n",
        "    # Parse command-line arguments\n",
        "    parser = argparse.ArgumentParser(description=\"Memory-based text generator\")\n",
        "    parser.add_argument(\"--classifier\", type=str, required=True, help=\"Path to the Timbl classifier file\")\n",
        "    parser.add_argument(\"--tokenizer\", type=str, required=True, help=\"Name of the Hugging Face tokenizer\")\n",
        "    parser.add_argument(\"--timbl_args\", type=str, required=True, help=\"Timbl arguments as a single string (e.g., '-a4 +D')\")\n",
        "    parser.add_argument(\"--verbosity\", type=int, default=0, help=\"Verbosity level (0: silent, 1: basic, 2: detailed, 3: debug)\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Set global verbosity level\n",
        "    print(f\"args.verbosity: {args.verbosity}\") # Debugging Print\n",
        "    VERBOSITY = args.verbosity\n",
        "    print(f\"VERBOSITY: {VERBOSITY}\") # Debugging Print\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
        "    log(f\"Tokenizer {tokenizer} loaded\", level=1)\n",
        "\n",
        "    # Initialize the Timbl classifier\n",
        "    classifier = timbl.TimblClassifier(args.classifier, args.timbl_args)\n",
        "    classifier.load()\n",
        "    log(\"TiMBL classifier loaded\", level=1)\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\"antalvdb/mblm-chatbot-instruction-prompts-igtree\")\n",
        "    tokenizer.add_special_tokens({'pad_token': '_'})\n",
        "    tokenizer.pad_token = \"_\"\n",
        "    #print(config)\n",
        "\n",
        "    # Initialize the TimblHuggingFaceModel\n",
        "    model = TimblHuggingFaceModel(config, classifier, tokenizer)  # Pass tokenizer\n",
        "\n",
        "    # Single prompt test:\n",
        "    # user_input = input(\"Please enter prompt: \")\n",
        "    # generate_text_from_api(model, tokenizer, user_input)\n",
        "\n",
        "    # Loop to continuously ask for input and classify\n",
        "    while True:\n",
        "        # Take input from the user\n",
        "        user_input = input(\"Please enter prompt (or type 'exit' to quit): \")\n",
        "\n",
        "        # Check if the user wants to exit\n",
        "        if user_input.lower() == 'exit':\n",
        "            log(\"Exiting.\", level=1)\n",
        "            break\n",
        "\n",
        "        # Pass the input to the classifier function\n",
        "        generate_text_from_api(model, tokenizer, user_input)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2wbIDd7S16k",
        "outputId": "e60a8e8e-06b1-4b30-9206-c0097443d8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.verbosity: 1\n",
            "VERBOSITY: 1\n",
            "Tokenizer BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            ") loaded\n",
            "TiMBL classifier loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calling Timbl API : -F Tabbed -a1 +D\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter prompt (or type 'exit' to quit): Who invented the telephone?\n",
            "Generated text: Who invented the telephone? Alexander Graham Bell is credited with inventing 2 6s to.. include 6K the I. the. include 6 Tsp timera, 6Krin )oast, and Giant include 6., 6 Out 6 other Set the and days up with a creative way to use the given word in a sentence elephant She felt like a small elephant in a room full of giant people. You should learn more about the meanings of words like nigger, 6eek, they, include 6ee treatment \" are we a. include of by \" the the the a \" the \", \" be \", \" explore \", \" great \", \" adventure \", \" ancient \", and \" treasure \". John was an adventurous, 6tensing and, include 6ing, 6est the in a. and, include best m I but a way I lot to me. I just want to spend as much time as I can with her, you could even bring a Nintendo switch to the hospital\n",
            "Please enter prompt (or type 'exit' to quit): How many planets are there?\n",
            "Generated text: How many planets are there? There are seven continents : Africa, Antarctica, Asia, Australia, Europe, North America, and South America, the Caribbean, and some parts of the United States, while French is spoken in France, Switzerland, Luxembourg, Belgium, Canada, several African countries including Algeria, Morocco, and touring the ancient ruins of Machu, 6P - and, include 6ri trap - even a. include 6 - if video. 8 you games This. torre, 6oastxi and, include 6mel of, 6 = - Alex in 6 #bourne 12 months include 6 `ot, :. `, 6mel be and data include 6 #bourne a how include 6 `otbonmon 6 `, 6mel be Pro 6eti. ` to, include a 6 The < ` but a no ` I lot of harm has been to people by their preferred gender. How do you think they feel if you refer to them by the Constitution, including the right to vote in all federal, state,\n",
            "Please enter prompt (or type 'exit' to quit): Why are the oceans blue?\n",
            "Generated text: Why are the oceans blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two base colours that are mixed to create blue? The two\n"
          ]
        }
      ]
    }
  ]
}