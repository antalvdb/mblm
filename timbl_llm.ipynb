{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antalvdb/mblm/blob/main/timbl_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq699BLeOfyz"
      },
      "source": [
        "# Memory-based language modeling\n",
        "\n",
        "Looking for an LLM that is relatively eco-friendly? MBLMs rely on CPUs. No GPUs or TPUs required. Training MBLMs is costly in terms of RAM, but not in terms of time or computing resources. Running an MBLM in autoregressive GPT-style mode also costs RAM, but still relies on CPUs and is reasonably fast as well, depending on the selected approximation of k-nearest neighbor classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tvCDBQZGNqO8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import re\n",
        "import time\n",
        "import argparse\n",
        "import sys\n",
        "import ast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxmB6txxNmjs"
      },
      "source": [
        "Installing `python3-timbl`, Python bindings for the TiMBL engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ-xeliyOBgd",
        "outputId": "94301994-8c72-4558-edab-74385acf0843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python3-timbl in /usr/local/lib/python3.11/dist-packages (2025.1.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install python3-timbl\n",
        "\n",
        "import timbl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYUDWeGkNtLi"
      },
      "source": [
        "Downloading an example MBLM model (approx. 457MB) with git lfs. This may take a while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRHCgzSgKymy",
        "outputId": "d88864b0-cfe8-4c74-c297-75748f5d7f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mblm'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 93 (delta 48), reused 26 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (93/93), 54.24 KiB | 1009.00 KiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n",
            "/root/mblm\n",
            "/root\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/antalvdb/mblm\n",
        "%cd mblm\n",
        "!git lfs pull -I chatbot-instruction-prompts_tok.l16r0.igtree.ibase\n",
        "%cd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_nihwUgN6sq"
      },
      "source": [
        "Setting a global verbosity level, and a function for logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "T2Oybcp2Scos"
      },
      "outputs": [],
      "source": [
        "# Global verbosity level\n",
        "VERBOSITY = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_APEp2gxSoIJ"
      },
      "outputs": [],
      "source": [
        "def log(message, level=1):\n",
        "    \"\"\"Logs a message if the verbosity level is sufficient.\"\"\"\n",
        "    if VERBOSITY >= level:\n",
        "        print(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AneliMC8OKQQ"
      },
      "source": [
        "Setting functions for padding the prompt with zeroes, and for generating text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1hG1i9xASuwt"
      },
      "outputs": [],
      "source": [
        "def pad_prompt(words, max_len=16):\n",
        "    \"\"\"Pad or trim the list of words to make it exactly `max_len` words.\"\"\"\n",
        "    if words is None:\n",
        "        words = []  # Ensure words is a list\n",
        "    if len(words) < max_len:\n",
        "        words = ['_'] * (max_len - len(words)) + words\n",
        "    else:\n",
        "        words = words[-max_len:]\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "08traJcDT3VC"
      },
      "outputs": [],
      "source": [
        "def generate_text_from_api(classifier, initial_prompt, max_words=200):\n",
        "    # Tokenize the initial prompt and convert tokens back to words\n",
        "    initial_tokens = tokenizer.tokenize(initial_prompt)\n",
        "\n",
        "    if initial_tokens is None:\n",
        "        log(\"Tokenization failed; 'initial_tokens' is None.\", level=1)\n",
        "        initial_tokens = []\n",
        "\n",
        "    # Prepare the initial prompt, padded or trimmed to 16 words\n",
        "    padded_instances = []\n",
        "\n",
        "    # Generate padded instances for next-token predictions\n",
        "    for i in range(len(initial_tokens)):\n",
        "        # Take the tokens up to the current position and pad them\n",
        "        instance = pad_prompt(initial_tokens[:i], max_len=16)\n",
        "        padded_instances.append((instance, initial_tokens[i] if i < len(initial_tokens) else '_'))\n",
        "\n",
        "    # Add instances to memory\n",
        "    for input_instance, next_token in padded_instances:\n",
        "        log(f\"memorized: {input_instance} {next_token}\", level=2)\n",
        "        classifier.append(input_instance, next_token)\n",
        "\n",
        "    # Use the final part of the prompt for further generation\n",
        "    prompt_words = pad_prompt(initial_tokens)\n",
        "\n",
        "    generated_tokens = prompt_words[:]  # Store the full generated text\n",
        "\n",
        "    try:\n",
        "        # Loop until max words generated or a period token is found\n",
        "        for _ in range(max_words):\n",
        "            next_word = None\n",
        "\n",
        "            classlabel, distribution, distance = classifier.classify(prompt_words)\n",
        "\n",
        "            # Add instance to instance base\n",
        "            classifier.append(prompt_words, classlabel)\n",
        "\n",
        "            log(f\"Prompt words: {prompt_words}\", level=2)\n",
        "            log(f\"Classlabel: {classlabel}\", level=2)\n",
        "            log(f\"Distribution: {distribution}\", level=3)\n",
        "            log(f\"Distance: {distance}\", level=3)\n",
        "\n",
        "            generated_tokens.append(classlabel)\n",
        "\n",
        "            # Shift prompt words and add the new word\n",
        "            prompt_words = prompt_words[1:] + [classlabel]\n",
        "\n",
        "            # Stop if a period is generated\n",
        "            # if classlabel == \".\":\n",
        "            #     break\n",
        "\n",
        "        # Detokenize the generated tokens\n",
        "        generated_text = tokenizer.convert_tokens_to_string(generated_tokens)\n",
        "\n",
        "        # Strip off original padding characters\n",
        "        generated_text = generated_text.replace(\"_\", \"\").strip()\n",
        "\n",
        "        # Print the final generated text\n",
        "        log(f\"Generated text: {generated_text}\", level=1)\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f\"Error: {e}\", level=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "D2wbIDd7S16k"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Simulate command-line arguments for notebook environment\n",
        "    sys.argv = ['script_name', '--classifier', '/content/mblm/chatbot-instruction-prompts-100k_tok.l16r0', '--tokenizer', 'bert-base-cased', '--timbl_args', '-a4 +D', '--verbosity', '3']\n",
        "\n",
        "    # Parse command-line arguments\n",
        "    parser = argparse.ArgumentParser(description=\"Memory-based text generator\")\n",
        "    parser.add_argument(\"--classifier\", type=str, required=True, help=\"Path to the Timbl classifier file\")\n",
        "    parser.add_argument(\"--tokenizer\", type=str, required=True, help=\"Name of the Hugging Face tokenizer\")\n",
        "    parser.add_argument(\"--timbl_args\", type=str, required=True, help=\"Timbl arguments as a single string (e.g., '-a4 +D')\")\n",
        "    parser.add_argument(\"--verbosity\", type=int, default=0, help=\"Verbosity level (0: silent, 1: basic, 2: detailed, 3: debug)\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Set global verbosity level\n",
        "    VERBOSITY = args.verbosity\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
        "\n",
        "    # Initialize the classifier\n",
        "    log(\"Loading TiMBL Classifier...\", level=1)\n",
        "    classifier = timbl.TimblClassifier(args.classifier, args.timbl_args)\n",
        "    classifier.load()\n",
        "\n",
        "    # Loop to continuously ask for input and classify\n",
        "    while True:\n",
        "        # Take input from the user\n",
        "        user_input = input(\"Please enter prompt (or type 'exit' to quit): \")\n",
        "\n",
        "        # Check if the user wants to exit\n",
        "        if user_input.lower() == 'exit':\n",
        "            log(\"Exiting.\", level=1)\n",
        "            break\n",
        "\n",
        "        # Pass the input to the classifier function\n",
        "        generate_text_from_api(classifier, user_input)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}