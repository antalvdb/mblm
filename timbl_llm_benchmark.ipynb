{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antalvdb/mblm/blob/main/timbl_llm_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wliCeZkDm1p-"
      },
      "source": [
        "# Benchmarking MBLEM\n",
        "\n",
        "##A notebook with a collection of repeatable benchmarks for MBLEM models\n",
        "\n",
        "This notebook contains a series of benchmarks that evaluate the Memory-Based Language Modeling (MBLM) on autoregression-based (decoder) tasks.\n",
        "\n",
        "MBLM is a CPU-based LLM, so Colab Runtime can be set to CPU.\n",
        "\n",
        "This Notebook is work in progress. Use with care!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydIh9K-7HRWi"
      },
      "source": [
        "##Firing up MBLM\n",
        "\n",
        "We begin with loading an `mblm` model. This requires installing `python3-timbl`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFk8Lop0h2i6",
        "outputId": "2dc614e6-860c-4efd-eca0-63a83a1a00e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python3-timbl\n",
            "  Downloading python3_timbl-2025.1.22-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (824 bytes)\n",
            "Downloading python3_timbl-2025.1.22-cp311-cp311-manylinux_2_28_x86_64.whl (21.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python3-timbl\n",
            "Successfully installed python3-timbl-2025.1.22\n"
          ]
        }
      ],
      "source": [
        "!pip install python3-timbl\n",
        "\n",
        "import timbl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyyB0tMzh6YD",
        "outputId": "cca0d80b-5744-4073-c2de-9c992d93bc9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mblm'...\n",
            "remote: Enumerating objects: 195, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 195 (delta 28), reused 8 (delta 8), pack-reused 147 (from 1)\u001b[K\n",
            "Receiving objects: 100% (195/195), 1.17 MiB | 1.30 MiB/s, done.\n",
            "Resolving deltas: 100% (107/107), done.\n",
            "/content/mblm\n",
            "/root\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/antalvdb/mblm\n",
        "%cd mblm\n",
        "!git lfs pull -I chatbot-instruction-prompts_tok.l16r0.igtree.ibase\n",
        "# !git lfs pull -I chatbot-instruction-prompts-100k_tok.l16r0.ibase\n",
        "%cd\n",
        "# Add the mblm directory to the Python path\n",
        "import sys\n",
        "sys.path.append('/content/mblm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL4IsN7Hwxic"
      },
      "source": [
        "## Benchmarking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwFPI4Qnz3F4"
      },
      "source": [
        "### Hellaswag\n",
        "\n",
        "Based on code by Andrew Karpathy, part of his [from-scratch reproduction of nanoGPT](https://github.com/karpathy/build-nanogpt/blob/master/hellaswag.py). Code was altered to run more optimally on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWtCe9qyd7r2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "#import tiktoken\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from IPython import get_ipython\n",
        "import numpy as np\n",
        "\n",
        "DATA_CACHE_DIR = os.path.join(os.getcwd(), \"hellaswag\") #Use os.getcwd() instead of os.path.dirname(__file__)\n",
        "\n",
        "def download_file(url: str, fname: str, chunk_size=1024):\n",
        "    \"\"\"Helper function to download a file from a given url\"\"\"\n",
        "    resp = requests.get(url, stream=True)\n",
        "    total = int(resp.headers.get(\"content-length\", 0))\n",
        "    with open(fname, \"wb\") as file, tqdm(\n",
        "        desc=fname,\n",
        "        total=total,\n",
        "        unit=\"iB\",\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024,\n",
        "    ) as bar:\n",
        "        for data in resp.iter_content(chunk_size=chunk_size):\n",
        "            size = file.write(data)\n",
        "            bar.update(size)\n",
        "\n",
        "hellaswags = {\n",
        "    \"train\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_train.jsonl\",\n",
        "    \"val\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl\",\n",
        "    \"test\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_test.jsonl\",\n",
        "}\n",
        "\n",
        "#enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def download(split):\n",
        "    \"\"\"Downloads HellaSwag DATA_CACHE_DIR\"\"\"\n",
        "    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
        "    data_url = hellaswags[split]\n",
        "    data_filename = os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\")\n",
        "    if not os.path.exists(data_filename):\n",
        "        print(f\"Downloading {data_url} to {data_filename}...\")\n",
        "        download_file(data_url, data_filename)\n",
        "\n",
        "# Initialize a dictionary to store unique context tokens and their tensor representations\n",
        "context_token_tensors = {}\n",
        "\n",
        "def render_example(example, tokenizer):\n",
        "    \"\"\"\n",
        "    Given the example as a dictionary, render it as three torch tensors:\n",
        "    - tokens (the tokens of context + completion, of size 4xN, as there are always 4 candidates)\n",
        "    - mask (is 1 in the region of the candidate completion, where we evaluate likelihoods)\n",
        "    - label (the index of the correct completion, which we hope has the highest likelihood)\n",
        "    \"\"\"\n",
        "    ctx = example[\"ctx\"]\n",
        "    label = example[\"label\"]\n",
        "    endings = example[\"endings\"]\n",
        "\n",
        "    # data needed to reproduce this eval on the C size\n",
        "    data = {\n",
        "        \"label\": label,\n",
        "        \"ctx_tokens\": None,\n",
        "        \"ending_tokens\": [],\n",
        "    }\n",
        "\n",
        "    # gather up all the tokens\n",
        "    ctx_tokens = tokenizer.encode(ctx, add_special_tokens=False)\n",
        "    data[\"ctx_tokens\"] = ctx_tokens\n",
        "\n",
        "    # Get or create the tensor for ctx_tokens\n",
        "    ctx_token_key = tuple(ctx_tokens)  # Convert to tuple for hashability\n",
        "    if ctx_token_key not in context_token_tensors:\n",
        "        context_token_tensors[ctx_token_key] = torch.tensor(ctx_tokens, dtype=torch.long)\n",
        "    ctx_tokens_tensor = context_token_tensors[ctx_token_key]\n",
        "\n",
        "    # Store token rows and mask rows as lists of tensors\n",
        "    token_tensors = []\n",
        "    mask_tensors = []\n",
        "\n",
        "    # Tokenize all endings at once\n",
        "    all_end_tokens = [tokenizer.encode(\" \" + end, add_special_tokens=False) for end in endings]\n",
        "\n",
        "    # Convert all_end_tokens to a tensor\n",
        "    # Instead of directly creating a tensor, we'll pad the sequences first\n",
        "    max_len = max(len(tokens) for tokens in all_end_tokens)\n",
        "    padded_end_tokens = [tokens + [0] * (max_len - len(tokens)) for tokens in all_end_tokens]\n",
        "    all_end_tokens_tensor = torch.tensor(padded_end_tokens, dtype=torch.long)\n",
        "\n",
        "    # Iterate through ending tokens\n",
        "    for i, end_tokens in enumerate(all_end_tokens):\n",
        "\n",
        "        # Get the corresponding tensor slice\n",
        "        end_tokens_tensor = all_end_tokens_tensor[i]\n",
        "\n",
        "        # Concatenate the tensors and append to the list\n",
        "        token_tensors.append(torch.cat([ctx_tokens_tensor, end_tokens_tensor]))\n",
        "\n",
        "        mask_tensors.append(torch.tensor([0] * len(ctx_tokens) + [1] * len(end_tokens), dtype=torch.long))\n",
        "        data[\"ending_tokens\"].append(end_tokens)\n",
        "\n",
        "    # Pad the tensors to the maximum length and stack them\n",
        "    max_len = max(len(tensor) for tensor in token_tensors)\n",
        "\n",
        "    padded_token_tensors = [F.pad(tensor, (0, max_len - len(tensor)), value=0) for tensor in token_tensors]\n",
        "    padded_mask_tensors = [F.pad(tensor, (0, max_len - len(tensor)), value=0) for tensor in mask_tensors]\n",
        "\n",
        "    # Instead of torch.stack, use torch.cat to concatenate along dimension 0\n",
        "    tokens = torch.cat(padded_token_tensors, dim=0)\n",
        "    mask = torch.cat(padded_mask_tensors, dim=0)\n",
        "    # Reshape to have 4 rows (candidates)\n",
        "    tokens = tokens.reshape(4, -1)\n",
        "    mask = mask.reshape(4, -1)\n",
        "\n",
        "    return data, tokens, mask, label\n",
        "\n",
        "def iterate_examples(split):\n",
        "    # there are 10,042 examples in total in val\n",
        "    download(split)\n",
        "    with open(os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\"), \"r\") as f:\n",
        "        for line in f:\n",
        "            example = json.loads(line)\n",
        "            yield example\n",
        "\n",
        "def evaluate_mblm_hellaswag(model, tokenizer, device):\n",
        "    num_correct_norm = 0\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for example in iterate_examples(\"val\"):\n",
        "        data, tokens, mask, label = render_example(example, tokenizer)\n",
        "        tokens = tokens.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        losses = []\n",
        "        for i in range(tokens.shape[0]): #iterate through the four options\n",
        "\n",
        "          input_ids = tokens[i].unsqueeze(0)\n",
        "\n",
        "          # Get the log probability of the sequence\n",
        "          logp = model.sequence_logprob(input_ids, tokenizer)\n",
        "          losses.append(-logp) # append the log probability to the losses list\n",
        "\n",
        "        # convert to tensor and find argmin\n",
        "        # losses = torch.tensor(losses)\n",
        "        # pred_norm = losses.argmin().item()\n",
        "\n",
        "        # Find the smallest non-zero loss and its index\n",
        "        min_loss = float('inf')\n",
        "        pred_norm = -1  # Initialize to an invalid index\n",
        "\n",
        "        for i, loss in enumerate(losses):\n",
        "            if loss > 0.0 and loss < min_loss:\n",
        "                min_loss = loss\n",
        "                pred_norm = i\n",
        "\n",
        "        # If no non-zero loss is found, pick randomly\n",
        "        if pred_norm == -1:\n",
        "            pred_norm = np.random.randint(0, 4)  # Pick random index between 0 and 3\n",
        "\n",
        "        # accumulate stats\n",
        "        num_total += 1\n",
        "        num_correct_norm += int(pred_norm == label)\n",
        "        log(\"---\", level = 1)\n",
        "        log(f\"{num_total} acc_norm: {num_correct_norm}/{num_total}={num_correct_norm/num_total:.4f}\", level = 1)\n",
        "\n",
        "        # debug: pretty print a few examples, and the losses in each case\n",
        "        # if num_total < 10:\n",
        "        log(\"---\", level = 1)\n",
        "        log(f\"Context:\\n {example['ctx']}\", level = 1)\n",
        "        log(f\"Endings:\", level = 1)\n",
        "        for i, end in enumerate(example[\"endings\"]):\n",
        "            log(f\"{i} (loss: {losses[i].item():.4f}) {end}\", level = 1)\n",
        "        log(f\"predicted: {pred_norm}, actual: {label}\", level = 1)\n",
        "\n",
        "        counter += 1\n",
        "        #if counter == 10:\n",
        "        #    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "exy1FQ6AgBk7",
        "outputId": "727b9ff1-008e-41a9-9a0f-ee8d9c008970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calling Timbl API : -F Tabbed -a1 +D\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'evaluate_mblm_hellaswag' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-85def7c328fd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimblHuggingFaceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mevaluate_mblm_hellaswag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate_mblm_hellaswag' is not defined"
          ]
        }
      ],
      "source": [
        "device = \"cpu\" # Set a default device for notebook execution\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import timbl\n",
        "import torch\n",
        "import mblm.mblm_model\n",
        "from mblm.mblm_model import TimblHuggingFaceModel, log, pad_prompt, pad_prompt_tokenids, log_probs_from_logits, log\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Initialize the Timbl classifier\n",
        "classifier = timbl.TimblClassifier('/content/mblm/chatbot-instruction-prompts_tok.l16r0.igtree', '-a1 +D')\n",
        "classifier.load()\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"antalvdb/mblm-chatbot-instruction-prompts-igtree\")\n",
        "tokenizer.add_special_tokens({'pad_token': '_'})\n",
        "tokenizer.pad_token = \"_\"\n",
        "\n",
        "# Initialize the TimblHuggingFaceModel\n",
        "model = TimblHuggingFaceModel(config, classifier, tokenizer)\n",
        "\n",
        "evaluate_mblm_hellaswag(model, tokenizer, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next-word prediction\n",
        "\n",
        "A benchmark on the simplest of all tasks: predicting the next word accurately."
      ],
      "metadata": {
        "id": "gDVdF55zC2E4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\" # Set a default device for notebook execution\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from IPython import get_ipython\n",
        "import numpy as np\n",
        "from mblm.mblm_model import pad_prompt, pad_prompt_tokenids  # Import necessary functions\n",
        "\n",
        "def evaluate_mblm_next_word_prediction(model, tokenizer, text_filepath, device):\n",
        "    \"\"\"Evaluates MBLM on next-word prediction using raw text from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        model: The MBLM model.\n",
        "        tokenizer: The tokenizer used for the model.\n",
        "        text_filepath: Path to the JSON file containing raw text.\n",
        "        device: The device to run the evaluation on (e.g., \"cpu\", \"cuda\").\n",
        "\n",
        "    Returns:\n",
        "        The accuracy of the model on next-word prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the raw text from the JSON file\n",
        "    with open(text_filepath, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Check if data is a list and handle accordingly\n",
        "    if isinstance(data, list):\n",
        "        # Assuming the text is the first element of the list\n",
        "        text = data[0].get(\"text\", \"\") if data else \"\"\n",
        "    else:\n",
        "        # If data is a dictionary, proceed as before\n",
        "        text = data.get(\"text\", \"\")\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    window_size = 16  # Set the sliding window size\n",
        "\n",
        "    # Start timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Process the text word by word\n",
        "    for i in range(len(tokenized_text) - 1):\n",
        "        # Get the current context and the next word (target)\n",
        "        # Use sliding window for context\n",
        "        context = tokenized_text[max(0, i - window_size + 1):i + 1]\n",
        "        target_word = tokenized_text[i + 1]\n",
        "\n",
        "        log(f\"Context: {' '.join(context)}\", level=3)\n",
        "        log(f\"Target word: {target_word}\", level=3)\n",
        "\n",
        "        # Convert context to input IDs\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(context)\n",
        "\n",
        "        # Pad the input_ids using pad_prompt_tokenids\n",
        "        padded_input_ids = pad_prompt_tokenids(input_ids, max_len=16, pad_token_id=tokenizer.pad_token_id)  # Pad to 64 tokens, using tokenizer's pad_token_id\n",
        "\n",
        "        # Convert padded_input_ids to a PyTorch tensor\n",
        "        input_ids = torch.tensor([padded_input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "        # Get MBLM's prediction for the next word\n",
        "        logits = model(input_ids).logits\n",
        "\n",
        "        # Reverted to using torch.argmax:\n",
        "        predicted_word_id = torch.argmax(logits[0, :]).item()  # Get the predicted word ID\n",
        "\n",
        "        # Convert the predicted word ID to a token\n",
        "        predicted_word = tokenizer.convert_ids_to_tokens(predicted_word_id)\n",
        "\n",
        "        log(f\"Predicted word: {predicted_word}\", level=3)\n",
        "\n",
        "        # Compare the prediction with the actual word\n",
        "        if predicted_word == target_word:\n",
        "            num_correct += 1\n",
        "\n",
        "        num_total += 1\n",
        "\n",
        "    # End timer\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate accuracy, handling division by zero\n",
        "    accuracy = num_correct / num_total if num_total else 0.0\n",
        "\n",
        "    # Calculate predictions per second\n",
        "    predictions_per_second = num_total / (end_time - start_time)\n",
        "\n",
        "    return accuracy, num_correct, num_total, predictions_per_second"
      ],
      "metadata": {
        "id": "Dkb4n-DuDAv5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\" # Set a default device for notebook execution\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import timbl\n",
        "import torch\n",
        "import mblm.mblm_model\n",
        "from mblm.mblm_model import TimblHuggingFaceModel, log, pad_prompt, pad_prompt_tokenids, log_probs_from_logits, log\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Initialize the Timbl classifier\n",
        "classifier = timbl.TimblClassifier('/content/mblm/chatbot-instruction-prompts_tok.l16r0.igtree', '-a1 +D')\n",
        "classifier.load()\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"antalvdb/mblm-chatbot-instruction-prompts-igtree\")\n",
        "tokenizer.add_special_tokens({'pad_token': '_'})\n",
        "tokenizer.pad_token = \"_\"\n",
        "\n",
        "# Initialize the TimblHuggingFaceModel\n",
        "model = TimblHuggingFaceModel(config, classifier, tokenizer)\n",
        "\n",
        "# Assuming you have an untokenized text file named \"untokenized_text.json\"\n",
        "text_filepath = \"/content/mblm/untokenized_text.json\"\n",
        "\n",
        "accuracy, num_correct, num_total, predictions_per_second = evaluate_gpt2_next_word_prediction(model, tokenizer, text_filepath, device)\n",
        "print(f\"Next-word prediction accuracy: {accuracy} ({num_correct} out of {num_total})\")\n",
        "print(f\"Predictions per second: {predictions_per_second:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfUgxkmGDZtQ",
        "outputId": "bb4a7634-b228-4f74-8c4a-41c639770ade"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calling Timbl API : -F Tabbed -a1 +D\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (961 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next-word prediction accuracy: 0.17291666666666666 (166 out of 960)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run the same evaluation, but now with GPT-2 small. Remember that this requires a GPU or TPU runtime engine and probably a fresh start or restart of this session."
      ],
      "metadata": {
        "id": "Hnzk58clnMxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import json\n",
        "import time\n",
        "\n",
        "def evaluate_gpt2_next_word_prediction(model, tokenizer, text_filepath, device):\n",
        "    \"\"\"Evaluates GPT-2 on next-word prediction using raw text from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        model: The GPT-2 model.\n",
        "        tokenizer: The tokenizer used for the model.\n",
        "        text_filepath: Path to the JSON file containing raw text.\n",
        "        device: The device to run the evaluation on (e.g., \"cpu\", \"cuda\").\n",
        "\n",
        "    Returns:\n",
        "        The accuracy of the model on next-word prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the raw text from the JSON file\n",
        "    with open(text_filepath, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Check if data is a list and handle accordingly\n",
        "    if isinstance(data, list):\n",
        "        # Assuming the text is the first element of the list\n",
        "        text = data[0].get(\"text\", \"\") if data else \"\"\n",
        "    else:\n",
        "        # If data is a dictionary, proceed as before\n",
        "        text = data.get(\"text\", \"\")\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    window_size = 16  # Set the sliding window size\n",
        "\n",
        "    # Start timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Process the text word by word\n",
        "    for i in range(len(tokenized_text) - 1):\n",
        "        # Get the current context and the next word (target)\n",
        "        # Use sliding window for context\n",
        "        context = tokenized_text[max(0, i - window_size + 1):i + 1]\n",
        "        target_word = tokenized_text[i + 1]\n",
        "\n",
        "        # Convert context to input IDs\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(context)\n",
        "\n",
        "        # Pad the input_ids to a fixed length, if necessary\n",
        "        # input_ids = input_ids[:window_size] # Truncate context if it's longer than window_size\n",
        "        # input_ids = input_ids + [tokenizer.pad_token_id] * (window_size - len(input_ids)) # Pad with pad_token_id\n",
        "\n",
        "        # Convert input_ids to a PyTorch tensor\n",
        "        input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "        # Get GPT-2's prediction for the next word\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Reverted to using torch.argmax:\n",
        "        predicted_word_id = torch.argmax(logits[0, -1, :]).item()  # Get the predicted word ID\n",
        "\n",
        "        # Convert the predicted word ID to a token\n",
        "        predicted_word = tokenizer.convert_ids_to_tokens(predicted_word_id)\n",
        "\n",
        "        # Compare the prediction with the actual word\n",
        "        if predicted_word == target_word:\n",
        "            num_correct += 1\n",
        "\n",
        "        num_total += 1\n",
        "\n",
        "    # End timer\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate accuracy, handling division by zero\n",
        "    accuracy = num_correct / num_total if num_total else 0.0\n",
        "\n",
        "    # Calculate predictions per second\n",
        "    predictions_per_second = num_total / (end_time - start_time)\n",
        "\n",
        "    return accuracy, num_correct, num_total, predictions_per_second\n",
        "\n",
        "# Load pre-trained GPT-2 small model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Assuming you have an untokenized text file named \"untokenized_text.json\"\n",
        "text_filepath = \"/content/mblm/untokenized_text.json\"\n",
        "\n",
        "# Evaluate on CPU by default\n",
        "device = \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "accuracy, num_correct, num_total, predictions_per_second = evaluate_gpt2_next_word_prediction(model, tokenizer, text_filepath, device)\n",
        "print(f\"Next-word prediction accuracy: {accuracy:.4f} ({num_correct} out of {num_total})\")\n",
        "print(f\"Predictions per second: {predictions_per_second:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vVuzT-xnMEP",
        "outputId": "f6c61af8-33a2-4bee-9f3f-8ac65597d688"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next-word prediction accuracy: 0.36 (355 out of 974)\n",
            "Predictions per second: 23.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kXPzejqxFON"
      },
      "source": [
        "### TriviaQA\n",
        "\n",
        "This is based on a [Colab Notebook](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb) by Patrick van Platen. This part of the notebook does not work yet and is very much work in progres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqJxYP1uy6eA"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "validation_dataset = datasets.load_dataset(\"trivia_qa\", \"rc\", split=\"validation[:5%]\")  # remove [:5%] to run on full validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt62J5U82QQR"
      },
      "outputs": [],
      "source": [
        "validation_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w62o4k3E2ZTp"
      },
      "outputs": [],
      "source": [
        "validation_dataset.info.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQHyhEzX2gIW"
      },
      "outputs": [],
      "source": [
        "def format_dataset(example):\n",
        "    # the context might be comprised of multiple contexts => me merge them here\n",
        "    example[\"context\"] = \" \".join((\"\\n\".join(example[\"entity_pages\"][\"wiki_context\"])).split(\"\\n\"))\n",
        "    example[\"targets\"] = example[\"answer\"][\"aliases\"]\n",
        "    example[\"norm_target\"] = example[\"answer\"][\"normalized_value\"]\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmKTbYLD2jLz"
      },
      "outputs": [],
      "source": [
        "validation_dataset = validation_dataset.map(format_dataset, remove_columns=[\"search_results\", \"question_source\", \"entity_pages\", \"answer\", \"question_id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0FnpD4U2oAc"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "impw1t_M2rjs"
      },
      "outputs": [],
      "source": [
        "show_random_elements(validation_dataset, num_examples=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFw2U2iE2xKn"
      },
      "outputs": [],
      "source": [
        "validation_dataset = validation_dataset.filter(lambda x: len(x[\"context\"]) > 0)\n",
        "# check out how many samples are left\n",
        "validation_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqx9YD1v3yIc"
      },
      "outputs": [],
      "source": [
        "short_validation_dataset = validation_dataset.filter(lambda x: (len(x['question']) + len(x['context'])) < 4 * 4096)\n",
        "short_validation_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1XOJf8a3DmD"
      },
      "outputs": [],
      "source": [
        "PUNCTUATION_SET_TO_EXCLUDE = set(''.join(['‘', '’', '´', '`', '.', ',', '-', '\"']))\n",
        "\n",
        "def get_sub_answers(answers, begin=0, end=None):\n",
        "  return [\" \".join(x.split(\" \")[begin:end]) for x in answers if len(x.split(\" \")) > 1]\n",
        "\n",
        "def expand_to_aliases(given_answers, make_sub_answers=False):\n",
        "  if make_sub_answers:\n",
        "    # if answers are longer than one word, make sure a predictions is correct if it coresponds to the complete 1: or :-1 sub word\n",
        "    # *e.g.* if the correct answer contains a prefix such as \"the\", or \"a\"\n",
        "    given_answers = given_answers + get_sub_answers(given_answers, begin=1) + get_sub_answers(given_answers, end=-1)\n",
        "  answers = []\n",
        "  for answer in given_answers:\n",
        "    alias = answer.replace('_', ' ').lower()\n",
        "    alias = ''.join(c if c not in PUNCTUATION_SET_TO_EXCLUDE else ' ' for c in alias)\n",
        "    answers.append(' '.join(alias.split()).strip())\n",
        "  return set(answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YNEvPqb3Sop"
      },
      "outputs": [],
      "source": [
        "def get_best_valid_start_end_idx(start_scores, end_scores, top_k=1, max_size=100):\n",
        "    best_start_scores, best_start_idx = torch.topk(start_scores, top_k)\n",
        "    best_end_scores, best_end_idx = torch.topk(end_scores, top_k)\n",
        "\n",
        "    widths = best_end_idx[:, None] - best_start_idx[None, :]\n",
        "    mask = torch.logical_or(widths < 0, widths > max_size)\n",
        "    scores = (best_end_scores[:, None] + best_start_scores[None, :]) - (1e8 * mask)\n",
        "    best_score = torch.argmax(scores).item()\n",
        "\n",
        "    return best_start_idx[best_score % top_k], best_end_idx[best_score // top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPY7J82S3Wxv"
      },
      "outputs": [],
      "source": [
        "def evaluate_mblm_triviaqa(example):\n",
        "    # encode question and context so that they are seperated by a tokenizer.sep_token and cut at max_length\n",
        "    encoding = tokenizer(example[\"question\"], example[\"context\"], return_tensors=\"pt\", max_length=4096, padding=\"max_length\", truncation=True)\n",
        "    input_ids = encoding.input_ids.to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start_scores, end_scores = model(input_ids=input_ids).to_tuple()\n",
        "\n",
        "    start_score, end_score = get_best_valid_start_end_idx(start_scores[0], end_scores[0], top_k=8, max_size=16)\n",
        "\n",
        "    # Let's convert the input ids back to actual tokens\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\n",
        "    answer_tokens = all_tokens[start_score: end_score + 1]\n",
        "\n",
        "    example[\"output\"] = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
        "    #.replace('\"', '')  # remove space prepending space token and remove unnecessary '\"'\n",
        "\n",
        "    answers = expand_to_aliases(example[\"targets\"], make_sub_answers=True)\n",
        "    predictions = expand_to_aliases([example[\"output\"]])\n",
        "\n",
        "    # if there is a common element, it's a match\n",
        "    example[\"match\"] = len(list(answers & predictions)) > 0\n",
        "\n",
        "    return example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LxS7A0o3qnZ"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\" # Set a default device for notebook execution\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import timbl\n",
        "import torch\n",
        "import mblm.mblm_model\n",
        "from mblm.mblm_model import TimblHuggingFaceModel, log, pad_prompt, pad_prompt_tokenids, log_probs_from_logits\n",
        "from mblm.mblm_model import log\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Initialize the Timbl classifier\n",
        "classifier = timbl.TimblClassifier('/content/mblm/chatbot-instruction-prompts_tok.l16r0.igtree', '-a1 +D')\n",
        "classifier.load()\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"antalvdb/mblm-chatbot-instruction-prompts-igtree\")\n",
        "tokenizer.add_special_tokens({'pad_token': '_'})\n",
        "tokenizer.pad_token = \"_\"\n",
        "\n",
        "# Initialize the TimblHuggingFaceModel\n",
        "model = TimblHuggingFaceModel(config, classifier, tokenizer)\n",
        "\n",
        "results_short = short_validation_dataset.map(evaluate_mblm_triviaqa)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}