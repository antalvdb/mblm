{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antalvdb/mblm/blob/main/timbl_llm_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wliCeZkDm1p-"
      },
      "source": [
        "# Benchmarking MBLEM\n",
        "\n",
        "##A notebook with a collection of repeatable benchmarks for MBLEM models\n",
        "\n",
        "This notebook contains a series of benchmarks that evaluate the Memory-Based Language Modeling (MBLM) on autoregression-based (decoder) tasks.\n",
        "\n",
        "MBLM is a CPU-based LLM, so Colab Runtime can be set to CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydIh9K-7HRWi"
      },
      "source": [
        "##Firing up MBLM\n",
        "\n",
        "We begin with loading an `mblm` model. This requires installing `python3-timbl`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFk8Lop0h2i6",
        "outputId": "71ebe85c-ed42-44cd-beee-3d16b4ff8e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python3-timbl in /usr/local/lib/python3.11/dist-packages (2025.1.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install python3-timbl\n",
        "\n",
        "import timbl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyyB0tMzh6YD",
        "outputId": "c8dd2d6a-d7b2-4282-b5cc-35e88e7d68f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mblm'...\n",
            "remote: Enumerating objects: 165, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 165 (delta 11), reused 8 (delta 8), pack-reused 147 (from 1)\u001b[K\n",
            "Receiving objects: 100% (165/165), 297.88 KiB | 6.34 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n",
            "/content/mblm\n",
            "/root\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/antalvdb/mblm\n",
        "%cd mblm\n",
        "!git lfs pull -I chatbot-instruction-prompts_tok.l16r0.igtree.ibase\n",
        "#!git lfs pull -I chatbot-instruction-prompts-100k_tok.l16r0.ibase\n",
        "%cd\n",
        "# Add the mblm directory to the Python path\n",
        "import sys\n",
        "sys.path.append('/content/mblm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL4IsN7Hwxic"
      },
      "source": [
        "## Benchmarking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwFPI4Qnz3F4"
      },
      "source": [
        "### Hellaswag\n",
        "\n",
        "Based on code by Andrew Karpathy, part of his [from-scratch reproduction of nanoGPT](https://github.com/karpathy/build-nanogpt/blob/master/hellaswag.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TWtCe9qyd7r2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "#import tiktoken\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from IPython import get_ipython\n",
        "\n",
        "DATA_CACHE_DIR = os.path.join(os.getcwd(), \"hellaswag\") #Use os.getcwd() instead of os.path.dirname(__file__)\n",
        "\n",
        "def download_file(url: str, fname: str, chunk_size=1024):\n",
        "    \"\"\"Helper function to download a file from a given url\"\"\"\n",
        "    resp = requests.get(url, stream=True)\n",
        "    total = int(resp.headers.get(\"content-length\", 0))\n",
        "    with open(fname, \"wb\") as file, tqdm(\n",
        "        desc=fname,\n",
        "        total=total,\n",
        "        unit=\"iB\",\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024,\n",
        "    ) as bar:\n",
        "        for data in resp.iter_content(chunk_size=chunk_size):\n",
        "            size = file.write(data)\n",
        "            bar.update(size)\n",
        "\n",
        "hellaswags = {\n",
        "    \"train\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_train.jsonl\",\n",
        "    \"val\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl\",\n",
        "    \"test\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_test.jsonl\",\n",
        "}\n",
        "\n",
        "#enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def download(split):\n",
        "    \"\"\"Downloads HellaSwag DATA_CACHE_DIR\"\"\"\n",
        "    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
        "    data_url = hellaswags[split]\n",
        "    data_filename = os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\")\n",
        "    if not os.path.exists(data_filename):\n",
        "        print(f\"Downloading {data_url} to {data_filename}...\")\n",
        "        download_file(data_url, data_filename)\n",
        "\n",
        "def render_example(example, tokenizer):\n",
        "    \"\"\"\n",
        "    Given the example as a dictionary, render it as three torch tensors:\n",
        "    - tokens (the tokens of context + completion, of size 4xN, as there are always 4 candidates)\n",
        "    - mask (is 1 in the region of the candidate completion, where we evaluate likelihoods)\n",
        "    - label (the index of the correct completion, which we hope has the highest likelihood)\n",
        "    \"\"\"\n",
        "    ctx = example[\"ctx\"]\n",
        "    label = example[\"label\"]\n",
        "    endings = example[\"endings\"]\n",
        "\n",
        "    # data needed to reproduce this eval on the C size\n",
        "    data = {\n",
        "        \"label\": label,\n",
        "        \"ctx_tokens\": None,\n",
        "        \"ending_tokens\": [],\n",
        "    }\n",
        "\n",
        "    # gather up all the tokens\n",
        "    ctx_tokens = tokenizer.encode(ctx, add_special_tokens=False)\n",
        "    data[\"ctx_tokens\"] = ctx_tokens\n",
        "    tok_rows = []\n",
        "    mask_rows = []\n",
        "    for end in endings:\n",
        "        end_tokens = tokenizer.encode(\" \" + end, add_special_tokens=False)  # note: prepending \" \" because GPT-2 tokenizer\n",
        "        tok_rows.append(ctx_tokens + end_tokens)\n",
        "        mask_rows.append([0]*len(ctx_tokens) + [1]*len(end_tokens))\n",
        "        data[\"ending_tokens\"].append(end_tokens)\n",
        "\n",
        "    # have to be careful during the collation because the number of tokens in each row can differ\n",
        "    max_len = max(len(row) for row in tok_rows)\n",
        "    tokens = torch.zeros((4, max_len), dtype=torch.long)\n",
        "    mask = torch.zeros((4, max_len), dtype=torch.long)\n",
        "    for i, (tok_row, mask_row) in enumerate(zip(tok_rows, mask_rows)):\n",
        "        tokens[i, :len(tok_row)] = torch.tensor(tok_row)\n",
        "        mask[i, :len(mask_row)] = torch.tensor(mask_row)\n",
        "\n",
        "    return data, tokens, mask, label\n",
        "\n",
        "def iterate_examples(split):\n",
        "    # there are 10,042 examples in total in val\n",
        "    download(split)\n",
        "    with open(os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\"), \"r\") as f:\n",
        "        for line in f:\n",
        "            example = json.loads(line)\n",
        "            yield example\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_mblm(model, tokenizer, device):\n",
        "    num_correct_norm = 0\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "\n",
        "    for example in iterate_examples(\"val\"):\n",
        "        data, tokens, mask, label = render_example(example, tokenizer)\n",
        "        tokens = tokens.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        losses = []\n",
        "        for i in range(tokens.shape[0]): #iterate through the four options\n",
        "\n",
        "          input_ids = tokens[i].unsqueeze(0)\n",
        "\n",
        "          # Get the log probability of the sequence\n",
        "          logp = model.sequence_logprob(input_ids, tokenizer)\n",
        "          losses.append(-logp) # append the log probability to the losses list\n",
        "\n",
        "        # convert to tensor and find argmin\n",
        "        # losses = torch.tensor(losses)\n",
        "        # pred_norm = losses.argmin().item()\n",
        "\n",
        "        # Find the smallest non-zero loss and its index\n",
        "        min_loss = float('inf')\n",
        "        pred_norm = -1  # Initialize to an invalid index\n",
        "\n",
        "        for i, loss in enumerate(losses):\n",
        "            if loss > 0.0 and loss < min_loss:\n",
        "                min_loss = loss\n",
        "                pred_norm = i\n",
        "\n",
        "        # If no non-zero loss is found, pick randomly\n",
        "        if pred_norm == -1:\n",
        "            pred_norm = np.random.randint(0, 4)  # Pick random index between 0 and 3\n",
        "\n",
        "        # accumulate stats\n",
        "        num_total += 1\n",
        "        num_correct_norm += int(pred_norm == label)\n",
        "        log(\"---\", level = 1)\n",
        "        log(f\"{num_total} acc_norm: {num_correct_norm}/{num_total}={num_correct_norm/num_total:.4f}\", level = 1)\n",
        "\n",
        "        # debug: pretty print a few examples, and the losses in each case\n",
        "        # if num_total < 10:\n",
        "        log(\"---\", level = 1)\n",
        "        log(f\"Context:\\n {example['ctx']}\", level = 1)\n",
        "        log(f\"Endings:\", level = 1)\n",
        "        for i, end in enumerate(example[\"endings\"]):\n",
        "            log(f\"{i} (loss: {losses[i].item():.4f}) {end}\", level = 1)\n",
        "        log(f\"predicted: {pred_norm}, actual: {label}\", level = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exy1FQ6AgBk7"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\" # Set a default device for notebook execution\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import timbl\n",
        "import torch\n",
        "import mblm.mblm_model\n",
        "from mblm.mblm_model import TimblHuggingFaceModel #import the model class\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Initialize the Timbl classifier\n",
        "classifier = timbl.TimblClassifier('/content/mblm/chatbot-instruction-prompts_tok.l16r0.igtree', '-a1 +D')\n",
        "classifier.load()\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"antalvdb/mblm-chatbot-instruction-prompts-igtree\")\n",
        "tokenizer.add_special_tokens({'pad_token': '_'})\n",
        "tokenizer.pad_token = \"_\"\n",
        "\n",
        "# Initialize the TimblHuggingFaceModel\n",
        "model = TimblHuggingFaceModel(config, classifier, tokenizer)\n",
        "\n",
        "evaluate_mblm(model, tokenizer, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kXPzejqxFON"
      },
      "source": [
        "### MMLU"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}