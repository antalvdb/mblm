{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antalvdb/mblm/blob/main/timbl_llm_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarking MBLEM\n",
        "\n",
        "##A notebook with a collection of repeatable benchmarks for MBLEM models\n",
        "\n",
        "This notebook contains a series of benchmarks that evaluate the Memory-Based Language Modeling (MBLM) on autoregression-based (decoder) tasks.\n",
        "\n",
        "MBLM is a CPU-based LLM, so Colab Runtime can be set to CPU."
      ],
      "metadata": {
        "id": "wliCeZkDm1p-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Firing up MBLM\n",
        "\n",
        "We begin with loading an `mblm` model. This requires installing `python3-timbl`.\n",
        "\n"
      ],
      "metadata": {
        "id": "ydIh9K-7HRWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python3-timbl\n",
        "\n",
        "import timbl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFk8Lop0h2i6",
        "outputId": "580cd5b0-b789-4fdd-b564-42e0efa7c16a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python3-timbl in /usr/local/lib/python3.11/dist-packages (2025.1.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/antalvdb/mblm\n",
        "%cd mblm\n",
        "!git lfs pull -I chatbot-instruction-prompts_tok.l16r0.igtree.ibase\n",
        "# !git lfs pull -I chatbot-instruction-prompts-100k_tok.l16r0.ibase\n",
        "%cd\n",
        "# Add the mblm directory to the Python path\n",
        "import sys\n",
        "sys.path.append('/content/mblm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyyB0tMzh6YD",
        "outputId": "2fdf7d86-beb0-4c4f-bce5-d4c31d7e166a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mblm'...\n",
            "remote: Enumerating objects: 150, done.\u001b[K\n",
            "remote: Counting objects: 100% (150/150), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "remote: Total 150 (delta 83), reused 26 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (150/150), 243.50 KiB | 3.16 MiB/s, done.\n",
            "Resolving deltas: 100% (83/83), done.\n",
            "/content/mblm\n",
            "/root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking"
      ],
      "metadata": {
        "id": "mL4IsN7Hwxic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hellaswag"
      ],
      "metadata": {
        "id": "OwFPI4Qnz3F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "#import tiktoken\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from IPython import get_ipython\n",
        "\n",
        "DATA_CACHE_DIR = os.path.join(os.getcwd(), \"hellaswag\") #Use os.getcwd() instead of os.path.dirname(__file__)\n",
        "\n",
        "def download_file(url: str, fname: str, chunk_size=1024):\n",
        "    \"\"\"Helper function to download a file from a given url\"\"\"\n",
        "    resp = requests.get(url, stream=True)\n",
        "    total = int(resp.headers.get(\"content-length\", 0))\n",
        "    with open(fname, \"wb\") as file, tqdm(\n",
        "        desc=fname,\n",
        "        total=total,\n",
        "        unit=\"iB\",\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024,\n",
        "    ) as bar:\n",
        "        for data in resp.iter_content(chunk_size=chunk_size):\n",
        "            size = file.write(data)\n",
        "            bar.update(size)\n",
        "\n",
        "hellaswags = {\n",
        "    \"train\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_train.jsonl\",\n",
        "    \"val\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl\",\n",
        "    \"test\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_test.jsonl\",\n",
        "}\n",
        "\n",
        "#enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def download(split):\n",
        "    \"\"\"Downloads HellaSwag DATA_CACHE_DIR\"\"\"\n",
        "    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
        "    data_url = hellaswags[split]\n",
        "    data_filename = os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\")\n",
        "    if not os.path.exists(data_filename):\n",
        "        print(f\"Downloading {data_url} to {data_filename}...\")\n",
        "        download_file(data_url, data_filename)\n",
        "\n",
        "def render_example(example, tokenizer):\n",
        "    \"\"\"\n",
        "    Given the example as a dictionary, render it as three torch tensors:\n",
        "    - tokens (the tokens of context + completion, of size 4xN, as there are always 4 candidates)\n",
        "    - mask (is 1 in the region of the candidate completion, where we evaluate likelihoods)\n",
        "    - label (the index of the correct completion, which we hope has the highest likelihood)\n",
        "    \"\"\"\n",
        "    ctx = example[\"ctx\"]\n",
        "    label = example[\"label\"]\n",
        "    endings = example[\"endings\"]\n",
        "\n",
        "    # data needed to reproduce this eval on the C size\n",
        "    data = {\n",
        "        \"label\": label,\n",
        "        \"ctx_tokens\": None,\n",
        "        \"ending_tokens\": [],\n",
        "    }\n",
        "\n",
        "    # gather up all the tokens\n",
        "    ctx_tokens = tokenizer.encode(ctx, add_special_tokens=False)\n",
        "    data[\"ctx_tokens\"] = ctx_tokens\n",
        "    tok_rows = []\n",
        "    mask_rows = []\n",
        "    for end in endings:\n",
        "        end_tokens = tokenizer.encode(\" \" + end, add_special_tokens=False)  # note: prepending \" \" because GPT-2 tokenizer\n",
        "        tok_rows.append(ctx_tokens + end_tokens)\n",
        "        mask_rows.append([0]*len(ctx_tokens) + [1]*len(end_tokens))\n",
        "        data[\"ending_tokens\"].append(end_tokens)\n",
        "\n",
        "    # have to be careful during the collation because the number of tokens in each row can differ\n",
        "    max_len = max(len(row) for row in tok_rows)\n",
        "    tokens = torch.zeros((4, max_len), dtype=torch.long)\n",
        "    mask = torch.zeros((4, max_len), dtype=torch.long)\n",
        "    for i, (tok_row, mask_row) in enumerate(zip(tok_rows, mask_rows)):\n",
        "        tokens[i, :len(tok_row)] = torch.tensor(tok_row)\n",
        "        mask[i, :len(mask_row)] = torch.tensor(mask_row)\n",
        "\n",
        "    return data, tokens, mask, label\n",
        "\n",
        "def iterate_examples(split):\n",
        "    # there are 10,042 examples in total in val\n",
        "    download(split)\n",
        "    with open(os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\"), \"r\") as f:\n",
        "        for line in f:\n",
        "            example = json.loads(line)\n",
        "            yield example\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_mblm(model, tokenizer, device):\n",
        "    num_correct_norm = 0\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "\n",
        "    for example in iterate_examples(\"val\"):\n",
        "        data, tokens, mask, label = render_example(example, tokenizer)\n",
        "        tokens = tokens.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        losses = []\n",
        "        for i in range(tokens.shape[0]): #iterate through the four options\n",
        "\n",
        "          input_ids = tokens[i].unsqueeze(0)\n",
        "\n",
        "          # Get the log probability of the sequence\n",
        "          logp = model.sequence_logprob(input_ids, tokenizer) #removed input_len argument\n",
        "          losses.append(-logp) # the lower the log prob, the higher the loss\n",
        "\n",
        "        losses = torch.tensor(losses)\n",
        "        pred_norm = losses.argmin().item()\n",
        "\n",
        "        # accumulate stats\n",
        "        num_total += 1\n",
        "        num_correct_norm += int(pred_norm == label)\n",
        "        print(f\"{num_total} acc_norm: {num_correct_norm}/{num_total}={num_correct_norm/num_total:.4f}\")\n",
        "\n",
        "        # debug: pretty print a few examples, and the losses in each case\n",
        "        # if num_total < 10:\n",
        "        print(\"---\")\n",
        "        print(f\"Context:\\n {example['ctx']}\")\n",
        "        print(f\"Endings:\")\n",
        "        for i, end in enumerate(example[\"endings\"]):\n",
        "            print(f\"{i} (loss: {losses[i].item():.4f}) {end}\")\n",
        "        print(f\"predicted: {pred_norm}, actual: {label}\")"
      ],
      "metadata": {
        "id": "TWtCe9qyd7r2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\" # Set a default device for notebook execution\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import timbl\n",
        "import torch\n",
        "import mblm.mblm_model\n",
        "from mblm.mblm_model import TimblHuggingFaceModel #import the model class\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Initialize the Timbl classifier\n",
        "classifier = timbl.TimblClassifier('/content/mblm/chatbot-instruction-prompts_tok.l16r0.igtree', '-a1 +D')\n",
        "classifier.load()\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"antalvdb/mblm-chatbot-instruction-prompts-igtree\")\n",
        "tokenizer.add_special_tokens({'pad_token': '_'})\n",
        "tokenizer.pad_token = \"_\"\n",
        "\n",
        "# Initialize the TimblHuggingFaceModel\n",
        "model = TimblHuggingFaceModel(config, classifier, tokenizer)\n",
        "\n",
        "evaluate_mblm(model, tokenizer, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exy1FQ6AgBk7",
        "outputId": "33c6d8c8-8b09-428b-e814-233de6fdeeb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calling Timbl API : -F Tabbed -a1 +D\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 acc_norm: 0/1=0.0000\n",
            "---\n",
            "Context:\n",
            " A man is sitting on a roof. he\n",
            "Endings:\n",
            "0 (loss: 30.0354) is using wrap to wrap a pair of skis.\n",
            "1 (loss: 21.3203) is ripping level tiles off.\n",
            "2 (loss: 35.1009) is holding a rubik's cube.\n",
            "3 (loss: 31.4388) starts pulling up roofing on a roof.\n",
            "predicted: 1, actual: 3\n",
            "2 acc_norm: 1/2=0.5000\n",
            "---\n",
            "Context:\n",
            " A lady walks to a barbell. She bends down and grabs the pole. the lady\n",
            "Endings:\n",
            "0 (loss: 34.6611) swings and lands in her arms.\n",
            "1 (loss: 35.4297) pulls the barbell forward.\n",
            "2 (loss: 45.9666) pulls a rope attached to the barbell.\n",
            "3 (loss: 32.5800) stands and lifts the weight over her head.\n",
            "predicted: 3, actual: 3\n",
            "3 acc_norm: 1/3=0.3333\n",
            "---\n",
            "Context:\n",
            " Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. the child and a different man\n",
            "Endings:\n",
            "0 (loss: 89.5754) are then shown paddling down a river in a boat while a woman talks.\n",
            "1 (loss: 80.3063) are driving the canoe, they go down the river flowing side to side.\n",
            "2 (loss: 87.0564) sit in a canoe while the man paddles.\n",
            "3 (loss: 99.6955) walking go down the rapids, while the man in his helicopter almost falls and goes out of canoehood.\n",
            "predicted: 1, actual: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MMLU"
      ],
      "metadata": {
        "id": "0kXPzejqxFON"
      }
    }
  ]
}